# -*- coding: utf-8 -*-
"""claude_adv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JTHabVDCjbvtzHbaEwle6aKL6HmK__AH
"""

# ADVANCED HYBRID PDF EXTRACTION SYSTEM
# Based on the enhanced hybrid approach from the analysis document

# Cell 1: Install Dependencies
!pip install pdfplumber pymupdf pandas numpy scikit-learn lightgbm matplotlib seaborn reportlab pillow regex

# Cell 2: Import Libraries and Setup
import os
import re
import glob
import json
import warnings
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from collections import defaultdict, Counter

import pdfplumber
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

# Report generation
from reportlab.pdfgen import canvas
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib.pagesizes import letter, A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from PIL import Image as PILImage, ImageDraw

warnings.filterwarnings('ignore')

# Cell 3: Data Classes and Configuration
@dataclass
class DocumentConfig:
    """Configuration for document processing"""
    SHORT_DOC_THRESHOLD: int = 5
    MEDIUM_DOC_THRESHOLD: int = 15
    LONG_DOC_THRESHOLD: int = 50

    CONFIDENCE_HIGH: float = 0.9
    CONFIDENCE_MEDIUM: float = 0.7
    CONFIDENCE_LOW: float = 0.5

    FONT_SIZE_BINS: int = 10
    POSITION_BINS: int = 20

@dataclass
class ExtractionResult:
    """Container for extraction results"""
    characters: List[Dict]
    blocks: List[Dict]
    lines: List[Dict]
    tables: List[Dict]
    images: List[Dict]
    structure: Dict
    confidence: float
    processing_method: str
    metadata: Dict

# Cell 4: Document Classification System
class DocumentClassifier:
    """Classifies documents by structure and complexity"""

    def __init__(self, config: DocumentConfig):
        self.config = config
        self.numbered_patterns = [
            r'^\s*(\d+(?:\.\d+)*)\s+[A-Z][A-Za-z\s]{2,50}',  # 1.2.3 Section
            r'^\s*([A-Z]\d+(?:\.\d+)*)\s+[A-Z][A-Za-z\s]{2,50}',  # A1.2 Appendix
            r'^\s*([IVX]+)\.\s+[A-Z][A-Za-z\s]{2,50}',  # Roman numerals
            r'^\s*([a-z])\)\s+[A-Z][A-Za-z\s]{2,50}',  # a) subsection
            r'^\s*\((\d+)\)\s+[A-Z][A-Za-z\s]{2,50}',  # (1) numbered
        ]

    def classify_document(self, pdf_path: str) -> Dict[str, Any]:
        """Classify document by page count and structure type"""

        classification = {
            'file_path': pdf_path,
            'page_count': 0,
            'size_category': 'unknown',
            'structure_type': 'unknown',
            'complexity_score': 0,
            'processing_strategy': 'fallback',
            'confidence': 0.0
        }

        try:
            with pdfplumber.open(pdf_path) as pdf:
                page_count = len(pdf.pages)
                classification['page_count'] = page_count

                # Classify by page count
                if page_count <= self.config.SHORT_DOC_THRESHOLD:
                    classification['size_category'] = 'short'
                elif page_count <= self.config.MEDIUM_DOC_THRESHOLD:
                    classification['size_category'] = 'medium'
                else:
                    classification['size_category'] = 'long'

                # Sample first few pages for structure analysis
                sample_pages = min(3, page_count)
                sample_text = ""

                for i in range(sample_pages):
                    page_text = pdf.pages[i].extract_text() or ""
                    sample_text += page_text + "\n"

                # Detect numbered structure
                numbered_matches = 0
                total_lines = len(sample_text.split('\n'))

                for line in sample_text.split('\n')[:50]:  # Check first 50 lines
                    line = line.strip()
                    if len(line) > 10:  # Skip very short lines
                        for pattern in self.numbered_patterns:
                            if re.match(pattern, line):
                                numbered_matches += 1
                                break

                # Determine structure type
                if numbered_matches >= 3:
                    classification['structure_type'] = 'numbered'
                    classification['processing_strategy'] = 'regex_primary'
                    classification['confidence'] = min(0.95, numbered_matches / 10)
                else:
                    classification['structure_type'] = 'unstructured'
                    classification['processing_strategy'] = 'ml_ensemble'
                    classification['confidence'] = 0.8

                # Calculate complexity score
                font_diversity = self._analyze_font_complexity(pdf)
                table_count = self._count_tables(pdf, sample_pages)
                image_count = self._count_images(pdf, sample_pages)

                complexity_score = (
                    font_diversity * 0.3 +
                    min(table_count * 0.2, 1.0) +
                    min(image_count * 0.1, 0.5) +
                    min(page_count / 100, 0.5)
                )

                classification['complexity_score'] = complexity_score

        except Exception as e:
            classification['error'] = str(e)
            classification['confidence'] = 0.0

        return classification

    def _analyze_font_complexity(self, pdf) -> float:
        """Analyze font diversity in document"""
        fonts = set()
        sizes = set()

        try:
            for page in pdf.pages[:3]:  # Sample first 3 pages
                for char in page.chars:
                    if char.get('fontname'):
                        fonts.add(char['fontname'])
                    if char.get('size'):
                        sizes.add(round(char['size'], 1))
        except:
            return 0.0

        return min(len(fonts) / 10 + len(sizes) / 20, 1.0)

    def _count_tables(self, pdf, max_pages: int) -> int:
        """Count tables in document sample"""
        table_count = 0
        try:
            for i in range(min(max_pages, len(pdf.pages))):
                tables = pdf.pages[i].find_tables()
                table_count += len(tables)
        except:
            pass
        return table_count

    def _count_images(self, pdf, max_pages: int) -> int:
        """Count images in document sample"""
        image_count = 0
        try:
            for i in range(min(max_pages, len(pdf.pages))):
                images = pdf.pages[i].images
                image_count += len(images)
        except:
            pass
        return image_count

# Cell 5: Feature Engineering System
class FeatureEngineer:
    """Advanced feature engineering for document elements"""

    def __init__(self, config: DocumentConfig):
        self.config = config
        self.scaler = StandardScaler()

    def engineer_features(self, elements: List[Dict], page_dims: Dict) -> pd.DataFrame:
        """Create ML-ready features from document elements"""

        if not elements:
            return pd.DataFrame()

        df = pd.DataFrame(elements)
        features_df = df.copy()

        # Document-adaptive normalization
        self._normalize_fonts(features_df)
        self._normalize_positions(features_df, page_dims)
        self._add_geometric_features(features_df)
        self._add_text_features(features_df)
        self._add_context_features(features_df)

        return features_df

    def _normalize_fonts(self, df: pd.DataFrame):
        """Normalize font sizes relative to document statistics"""
        if 'size' in df.columns and len(df) > 0:
            sizes = df['size'].dropna()
            if len(sizes) > 0:
                doc_mean = sizes.mean()
                doc_std = sizes.std() if sizes.std() > 0 else 1.0
                doc_max = sizes.max()

                df['size_normalized'] = (df['size'] - doc_mean) / doc_std
                df['size_percentile'] = df['size'].rank(pct=True)
                df['size_ratio_to_max'] = df['size'] / doc_max if doc_max > 0 else 0

    def _normalize_positions(self, df: pd.DataFrame, page_dims: Dict):
        """Normalize positions relative to page dimensions"""
        if 'x0' in df.columns and 'top' in df.columns:
            for idx, row in df.iterrows():
                page_num = row.get('page', 1)
                page_dim = page_dims.get(page_num, {'width': 612, 'height': 792})

                df.at[idx, 'x0_norm'] = row.get('x0', 0) / page_dim['width']
                df.at[idx, 'x1_norm'] = row.get('x1', 0) / page_dim['width']
                df.at[idx, 'top_norm'] = row.get('top', 0) / page_dim['height']
                df.at[idx, 'bottom_norm'] = row.get('bottom', 0) / page_dim['height']

    def _add_geometric_features(self, df: pd.DataFrame):
        """Add geometric relationship features"""
        if all(col in df.columns for col in ['x0', 'x1', 'top', 'bottom']):
            df['width'] = df['x1'] - df['x0']
            df['height'] = df['bottom'] - df['top']
            df['area'] = df['width'] * df['height']
            df['aspect_ratio'] = df['width'] / (df['height'] + 1e-6)

            # Center points
            df['center_x'] = (df['x0'] + df['x1']) / 2
            df['center_y'] = (df['top'] + df['bottom']) / 2

    def _add_text_features(self, df: pd.DataFrame):
        """Add text-based features"""
        if 'text' in df.columns:
            df['text_length'] = df['text'].str.len()
            df['word_count'] = df['text'].str.split().str.len()
            df['is_all_caps'] = df['text'].str.isupper()
            df['is_title_case'] = df['text'].str.istitle()
            df['has_numbers'] = df['text'].str.contains(r'\d')
            df['has_special_chars'] = df['text'].str.contains(r'[^\w\s]')
            df['starts_with_number'] = df['text'].str.match(r'^\d')

    def _add_context_features(self, df: pd.DataFrame):
        """Add contextual features based on surrounding elements"""
        if len(df) > 1:
            # Sort by page and position
            df_sorted = df.sort_values(['page', 'top', 'x0'])

            # Distance to previous/next elements
            df['prev_distance_y'] = df_sorted['top'].diff()
            df['next_distance_y'] = df_sorted['top'].diff(-1).abs()

            # Alignment features
            df['left_aligned'] = df['x0'].round() == df['x0'].round().mode().iloc[0] if len(df) > 0 else False

# Cell 6: Multi-Level Processing System
class HybridProcessor:
    """Main processing system with multiple fallback levels"""

    def __init__(self, config: DocumentConfig):
        self.config = config
        self.classifier = DocumentClassifier(config)
        self.feature_engineer = FeatureEngineer(config)
        self.ml_models = self._initialize_models()

    def _initialize_models(self) -> Dict:
        """Initialize ML models for ensemble"""
        return {
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                verbose=-1
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=50,
                max_depth=10,
                random_state=42
            ),
            'logistic': LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        }

    def process_document(self, pdf_path: str) -> ExtractionResult:
        """Main document processing pipeline"""

        # Step 1: Classify document
        classification = self.classifier.classify_document(pdf_path)

        # Step 2: Choose processing strategy
        if classification['processing_strategy'] == 'regex_primary':
            result = self._process_numbered_document(pdf_path, classification)
        else:
            result = self._process_unstructured_document(pdf_path, classification)

        # Step 3: Apply fallbacks if confidence is low
        if result.confidence < self.config.CONFIDENCE_MEDIUM:
            result = self._apply_fallback_processing(pdf_path, result, classification)

        return result

    def _process_numbered_document(self, pdf_path: str, classification: Dict) -> ExtractionResult:
        """Process documents with numbered structure using regex"""

        characters = []
        blocks = []
        lines = []
        tables = []
        images = []
        structure = {'sections': [], 'hierarchy': []}

        try:
            with pdfplumber.open(pdf_path) as pdf:
                page_dims = {i+1: {'width': p.width, 'height': p.height}
                           for i, p in enumerate(pdf.pages)}

                # Extract all content
                for page_num, page in enumerate(pdf.pages, 1):
                    # Characters
                    for char in page.chars:
                        char_data = {
                            'page': page_num,
                            'text': char.get('text', ''),
                            'fontname': char.get('fontname', ''),
                            'size': char.get('size', 0),
                            'x0': char.get('x0', 0),
                            'x1': char.get('x1', 0),
                            'top': char.get('top', 0),
                            'bottom': char.get('bottom', 0)
                        }
                        characters.append(char_data)

                    # Tables
                    for table_idx, table in enumerate(page.find_tables()):
                        table_data = {
                            'page': page_num,
                            'table_id': f"t_{page_num}_{table_idx}",
                            'bbox': table.bbox,
                            'data': table.extract()
                        }
                        tables.append(table_data)

                    # Images
                    for img_idx, img in enumerate(page.images):
                        img_data = {
                            'page': page_num,
                            'image_id': f"i_{page_num}_{img_idx}",
                            'bbox': [img.get('x0', 0), img.get('top', 0),
                                   img.get('x1', 0), img.get('bottom', 0)]
                        }
                        images.append(img_data)

                # Reconstruct lines and detect structure
                if characters:
                    lines = self._reconstruct_lines(characters)
                    structure = self._detect_numbered_structure(lines)

                # Extract blocks with PyMuPDF
                blocks = self._extract_blocks_pymupdf(pdf_path)

            confidence = min(0.95, len(structure.get('sections', [])) / 10)

        except Exception as e:
            confidence = 0.0
            structure['error'] = str(e)

        return ExtractionResult(
            characters=characters,
            blocks=blocks,
            lines=lines,
            tables=tables,
            images=images,
            structure=structure,
            confidence=confidence,
            processing_method='regex_primary',
            metadata=classification
        )

    def _process_unstructured_document(self, pdf_path: str, classification: Dict) -> ExtractionResult:
        """Process unstructured documents using ML ensemble"""

        characters = []
        blocks = []
        lines = []
        tables = []
        images = []
        structure = {'elements': [], 'predictions': []}

        try:
            with pdfplumber.open(pdf_path) as pdf:
                page_dims = {i+1: {'width': p.width, 'height': p.height}
                           for i, p in enumerate(pdf.pages)}

                # Extract content (similar to numbered approach)
                for page_num, page in enumerate(pdf.pages, 1):
                    for char in page.chars:
                        char_data = {
                            'page': page_num,
                            'text': char.get('text', ''),
                            'fontname': char.get('fontname', ''),
                            'size': char.get('size', 0),
                            'x0': char.get('x0', 0),
                            'x1': char.get('x1', 0),
                            'top': char.get('top', 0),
                            'bottom': char.get('bottom', 0)
                        }
                        characters.append(char_data)

                    # Tables and images
                    for table_idx, table in enumerate(page.find_tables()):
                        tables.append({
                            'page': page_num,
                            'table_id': f"t_{page_num}_{table_idx}",
                            'bbox': table.bbox,
                            'data': table.extract()
                        })

                    for img_idx, img in enumerate(page.images):
                        images.append({
                            'page': page_num,
                            'image_id': f"i_{page_num}_{img_idx}",
                            'bbox': [img.get('x0', 0), img.get('top', 0),
                                   img.get('x1', 0), img.get('bottom', 0)]
                        })

                # Process with ML
                if characters:
                    lines = self._reconstruct_lines(characters)
                    structure = self._classify_elements_ml(lines, page_dims)

                blocks = self._extract_blocks_pymupdf(pdf_path)

            confidence = 0.85  # Base confidence for ML approach

        except Exception as e:
            confidence = 0.0
            structure['error'] = str(e)

        return ExtractionResult(
            characters=characters,
            blocks=blocks,
            lines=lines,
            tables=tables,
            images=images,
            structure=structure,
            confidence=confidence,
            processing_method='ml_ensemble',
            metadata=classification
        )

    def _reconstruct_lines(self, characters: List[Dict]) -> List[Dict]:
        """Reconstruct text lines from characters"""
        if not characters:
            return []

        df_chars = pd.DataFrame(characters)
        df_chars['line_key'] = df_chars.apply(
            lambda x: f"{x['page']}_{round(x['top'])}", axis=1
        )

        lines = []
        for line_key, group in df_chars.groupby('line_key'):
            group_sorted = group.sort_values('x0')
            line_data = {
                'page': group_sorted.iloc[0]['page'],
                'text': ''.join(group_sorted['text']),
                'x0': group_sorted['x0'].min(),
                'x1': group_sorted['x1'].max(),
                'top': group_sorted['top'].min(),
                'bottom': group_sorted['bottom'].max(),
                'fontname': group_sorted['fontname'].mode().iloc[0] if len(group_sorted) > 0 else '',
                'size': group_sorted['size'].mean()
            }
            lines.append(line_data)

        return lines

    def _detect_numbered_structure(self, lines: List[Dict]) -> Dict:
        """Detect hierarchical structure in numbered documents"""
        structure = {'sections': [], 'hierarchy': []}

        for line in lines:
            text = line.get('text', '').strip()
            for pattern in self.classifier.numbered_patterns:
                match = re.match(pattern, text)
                if match:
                    section_data = {
                        'number': match.group(1),
                        'title': text[match.end():].strip(),
                        'page': line.get('page'),
                        'position': line.get('top'),
                        'level': len(match.group(1).split('.'))
                    }
                    structure['sections'].append(section_data)
                    break

        return structure

    def _classify_elements_ml(self, lines: List[Dict], page_dims: Dict) -> Dict:
        """Classify document elements using ML ensemble"""

        if not lines:
            return {'elements': [], 'predictions': []}

        # Engineer features
        features_df = self.feature_engineer.engineer_features(lines, page_dims)

        # Create synthetic labels for demonstration (in real scenario, use trained models)
        predictions = []
        for idx, row in features_df.iterrows():
            # Simple rule-based classification for demonstration
            if row.get('size_percentile', 0) > 0.8:
                label = 'header'
            elif row.get('size_percentile', 0) > 0.6:
                label = 'subheader'
            elif row.get('text_length', 0) < 10:
                label = 'short_text'
            else:
                label = 'body_text'

            predictions.append({
                'element_id': idx,
                'predicted_label': label,
                'confidence': 0.8,  # Placeholder confidence
                'features': row.to_dict()
            })

        return {'elements': lines, 'predictions': predictions}

    def _extract_blocks_pymupdf(self, pdf_path: str) -> List[Dict]:
        """Extract blocks using PyMuPDF"""
        blocks = []

        try:
            doc = fitz.open(pdf_path)
            for page_idx in range(len(doc)):
                page = doc[page_idx]
                page_dict = page.get_text("dict")

                for block in page_dict.get("blocks", []):
                    if "lines" not in block:
                        continue

                    bbox = block.get("bbox", [0, 0, 0, 0])
                    for line in block["lines"]:
                        for span in line.get("spans", []):
                            block_data = {
                                'page': page_idx + 1,
                                'text': span.get('text', ''),
                                'font': span.get('font', ''),
                                'size': span.get('size', 0),
                                'bbox': span.get('bbox', [0, 0, 0, 0]),
                                'block_bbox': bbox
                            }
                            blocks.append(block_data)
            doc.close()
        except Exception as e:
            print(f"Error extracting blocks: {e}")

        return blocks

    def _apply_fallback_processing(self, pdf_path: str, result: ExtractionResult,
                                 classification: Dict) -> ExtractionResult:
        """Apply fallback processing for low-confidence results"""

        # Try alternative extraction method
        try:
            if result.processing_method == 'regex_primary':
                # Fallback to ML approach
                fallback_result = self._process_unstructured_document(pdf_path, classification)
            else:
                # Fallback to basic rule-based approach
                fallback_result = self._basic_rule_based_processing(pdf_path, classification)

            # Use better result based on confidence and content
            if fallback_result.confidence > result.confidence:
                fallback_result.processing_method += '_fallback'
                return fallback_result

        except Exception as e:
            result.metadata['fallback_error'] = str(e)

        return result

    def _basic_rule_based_processing(self, pdf_path: str, classification: Dict) -> ExtractionResult:
        """Basic rule-based processing as final fallback"""

        characters = []
        lines = []
        structure = {'basic_analysis': {}}

        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    for char in page.chars:
                        characters.append({
                            'page': page_num,
                            'text': char.get('text', ''),
                            'size': char.get('size', 0),
                            'x0': char.get('x0', 0),
                            'top': char.get('top', 0)
                        })

                if characters:
                    lines = self._reconstruct_lines(characters)

                    # Basic structure analysis
                    sizes = [line.get('size', 0) for line in lines]
                    if sizes:
                        size_mean = np.mean(sizes)
                        size_std = np.std(sizes)

                        headers = [line for line in lines
                                 if line.get('size', 0) > size_mean + size_std]

                        structure['basic_analysis'] = {
                            'total_lines': len(lines),
                            'potential_headers': len(headers),
                            'avg_font_size': size_mean
                        }

        except Exception as e:
            structure['error'] = str(e)

        return ExtractionResult(
            characters=characters,
            blocks=[],
            lines=lines,
            tables=[],
            images=[],
            structure=structure,
            confidence=0.6,
            processing_method='basic_rules',
            metadata=classification
        )

# Cell 7: Test PDF Generation System
class TestPDFGenerator:
    """Generate comprehensive test PDFs for system validation"""

    def __init__(self, output_dir: str = "hybrid_test_pdfs"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def generate_all_test_pdfs(self) -> List[str]:
        """Generate all test PDF varieties"""

        generated_files = []

        # 1. Numbered structure document
        generated_files.append(self._create_numbered_document())

        # 2. Unstructured document with varied fonts
        generated_files.append(self._create_unstructured_document())

        # 3. Complex multi-column academic paper
        generated_files.append(self._create_academic_paper())

        # 4. Table-heavy financial report
        generated_files.append(self._create_financial_report())

        # 5. Mixed content presentation
        generated_files.append(self._create_presentation_doc())

        print(f"Generated {len(generated_files)} test PDFs in '{self.output_dir}':")
        for file in generated_files:
            print(f"  - {os.path.basename(file)}")

        return generated_files

    def _create_numbered_document(self) -> str:
        """Create document with clear numbered structure"""

        filepath = os.path.join(self.output_dir, "numbered_structure.pdf")
        c = canvas.Canvas(filepath, pagesize=letter)
        width, height = letter

        # Title
        c.setFont("Helvetica-Bold", 18)
        c.drawString(50, height-50, "Technical Specification Document")

        # Numbered sections
        sections = [
            ("1.", "Introduction", [
                "1.1 Purpose and Scope",
                "1.2 Document Overview",
                "1.3 Definitions and Abbreviations"
            ]),
            ("2.", "System Architecture", [
                "2.1 Overall Architecture",
                "2.2 Component Design",
                "2.3 Interface Specifications",
                "2.4 Data Flow Diagrams"
            ]),
            ("3.", "Implementation Details", [
                "3.1 Development Environment",
                "3.2 Coding Standards",
                "3.3 Testing Procedures"
            ]),
            ("4.", "Appendices", [
                "A.1 Configuration Files",
                "A.2 API Documentation",
                "B.1 Performance Benchmarks"
            ])
        ]

        y_pos = height - 100
        for section_num, section_title, subsections in sections:
            # Main section
            c.setFont("Helvetica-Bold", 14)
            c.drawString(50, y_pos, f"{section_num} {section_title}")
            y_pos -= 25

            # Subsections
            c.setFont("Helvetica", 11)
            for subsection in subsections:
                c.drawString(70, y_pos, subsection)
                y_pos -= 18

                # Add some body text
                c.setFont("Helvetica", 9)
                c.drawString(90, y_pos, "This section contains detailed information about the topic.")
                y_pos -= 15
                c.setFont("Helvetica", 11)

            y_pos -= 10

            # New page if needed
            if y_pos < 100:
                c.showPage()
                y_pos = height - 50

        c.save()
        return filepath

    def _create_unstructured_document(self) -> str:
        """Create unstructured document with font variations"""

        filepath = os.path.join(self.output_dir, "unstructured_varied.pdf")
        c = canvas.Canvas(filepath, pagesize=letter)
        width, height = letter

        # Mix of fonts and sizes without clear structure
        content_blocks = [
            ("Helvetica-Bold", 16, "Executive Summary Report"),
            ("Times-Roman", 12, "This document presents findings from our comprehensive analysis"),
            ("Courier", 10, "Data collected: 2024-Q4 period"),
            ("Helvetica", 14, "Key Findings"),
            ("Times-Roman", 11, "The analysis revealed several important trends in market behavior."),
            ("Helvetica-Oblique", 10, "Note: All data points have been verified for accuracy"),
            ("Courier-Bold", 12, "CONFIDENTIAL - Internal Use Only"),
            ("Times-Roman", 12, "Market trends show significant growth in the technology sector."),
            ("Helvetica", 13, "Recommendations"),
            ("Times-Roman", 11, "Based on our findings, we recommend the following actions:"),
            ("Courier", 9, "• Increase investment in R&D by 15%"),
            ("Courier", 9, "• Expand market presence in emerging economies"),
            ("Courier", 9, "• Develop strategic partnerships with key players"),
            ("Helvetica-Bold", 12, "Risk Assessment"),
            ("Times-Roman", 10, "Market volatility remains a primary concern for stakeholders."),
            ("Helvetica", 11, "Implementation Timeline"),
            ("Courier", 10, "Phase 1: Q1 2025 - Initial deployment"),
            ("Courier", 10, "Phase 2: Q2 2025 - Full rollout"),
            ("Times-Roman", 12, "Conclusion"),
            ("Times-Roman", 11, "This analysis provides a roadmap for future growth and development.")
        ]

        y_pos = height - 50
        for font, size, text in content_blocks:
            c.setFont(font, size)

            # Handle text wrapping
            words = text.split()
            line = ""
            for word in words:
                test_line = line + word + " "
                if c.stringWidth(test_line, font, size) > width - 100:
                    if line:
                        c.drawString(50, y_pos, line.strip())
                        y_pos -= size + 5
                        line = word + " "
                    else:
                        c.drawString(50, y_pos, word)
                        y_pos -= size + 5
                        line = ""
                else:
                    line = test_line

            if line:
                c.drawString(50, y_pos, line.strip())
                y_pos -= size + 8

            if y_pos < 100:
                c.showPage()
                y_pos = height - 50

        c.save()
        return filepath

    def _create_academic_paper(self) -> str:
        """Create multi-column academic paper"""

        filepath = os.path.join(self.output_dir, "academic_paper.pdf")
        c = canvas.Canvas(filepath, pagesize=A4)
        width, height = A4

        # Title and metadata
        c.setFont("Helvetica-Bold", 16)
        title = "Advanced Machine Learning Approaches for Document Structure Analysis"
        c.drawString(50, height-50, title)

        c.setFont("Helvetica-Oblique", 12)
        c.drawString(50, height-75, "Authors: Dr. Sarah Johnson¹, Prof. Michael Chen², Dr. Emily Rodriguez³")

        c.setFont("Helvetica", 8)
        c.drawString(50, height-90, "¹University of Technology, ²Research Institute, ³Data Science Lab")

        # Abstract
        c.setFont("Helvetica-Bold", 11)
        c.drawString(50, height-120, "Abstract")

        c.setFont("Helvetica", 9)
        abstract_lines = [
            "This paper presents novel machine learning methodologies for automated document",
            "structure extraction and analysis. Our hybrid approach combines rule-based pattern",
            "recognition with ensemble learning techniques to achieve 95% accuracy across diverse",
            "document types. Experimental results demonstrate significant improvements over",
            "traditional methods, particularly for complex multi-column layouts and hierarchical",
            "document structures. The proposed system is evaluated on a comprehensive dataset",
            "of 10,000+ documents spanning academic papers, technical reports, and legal documents."
        ]

        y_pos = height - 140
        for line in abstract_lines:
            c.drawString(50, y_pos, line)
            y_pos -= 12

        # Two-column content
        col1_x, col2_x = 50, 320
        col_width = 250

        # Column 1 content
        c.setFont("Helvetica-Bold", 10)
        c.drawString(col1_x, height-220, "1. Introduction")

        c.setFont("Helvetica", 8)
        intro_text = [
            "Document structure extraction represents a",
            "fundamental challenge in information retrieval",
            "and natural language processing. Traditional",
            "approaches rely heavily on heuristic rules",
            "and manual feature engineering, limiting their",
            "applicability across diverse document formats.",
            "",
            "Recent advances in machine learning, particularly",
            "deep learning architectures, have shown promise",
            "in addressing these limitations. However, most",
            "existing solutions focus on specific document",
            "types or require extensive labeled training data.",
            "",
            "This work proposes a hybrid methodology that",
            "combines the interpretability of rule-based",
            "systems with the adaptability of modern ML",
            "approaches, achieving robust performance across",
            "varied document structures without requiring",
            "domain-specific training."
        ]

        y_pos = height - 240
        for line in intro_text:
            if line == "":
                y_pos -= 8
            else:
                c.drawString(col1_x, y_pos, line)
                y_pos -= 10

        # Column 2 content
        c.setFont("Helvetica-Bold", 10)
        c.drawString(col2_x, height-220, "2. Related Work")

        c.setFont("Helvetica", 8)
        related_text = [
            "Prior research in document structure analysis",
            "can be categorized into three main approaches:",
            "rule-based methods [1,2], statistical approaches",
            "[3,4], and deep learning techniques [5,6].",
            "",
            "Rule-based systems excel at processing documents",
            "with consistent formatting but struggle with",
            "layout variations. Statistical methods provide",
            "better generalization but require careful",
            "feature engineering for optimal performance.",
            "",
            "Deep learning approaches, while powerful,",
            "often lack interpretability and require",
            "substantial computational resources. Our",
            "hybrid approach addresses these limitations",
            "by intelligently combining the strengths",
            "of each methodology while mitigating their",
            "individual weaknesses."
        ]

        y_pos = height-240
        for line in related_text:
            if line == "":
                y_pos -= 8
            else:
                c.drawString(col2_x, y_pos, line)
                y_pos -= 10

        # Footnotes
        c.setFont("Helvetica", 7)
        c.drawString(50, 50, "[1] Smith et al. 2023. Rule-based document parsing. ACL.")
        c.drawString(50, 40, "[2] Johnson 2022. Hierarchical structure detection. ICML.")
        c.drawString(50, 30, "[3] Chen et al. 2024. Statistical layout analysis. ICLR.")

        c.save()
        return filepath

    def _create_financial_report(self) -> str:
        """Create table-heavy financial report"""

        filepath = os.path.join(self.output_dir, "financial_tables.pdf")
        doc = SimpleDocTemplate(filepath, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []

        # Title
        elements.append(Paragraph("Quarterly Financial Report - Q4 2024", styles['Title']))
        elements.append(Spacer(1, 20))

        # Revenue table
        elements.append(Paragraph("Revenue Breakdown by Division", styles['Heading2']))
        elements.append(Spacer(1, 10))

        revenue_data = [
            ["Division", "Q1 2024", "Q2 2024", "Q3 2024", "Q4 2024", "YoY Growth"],
            ["Technology", "$2.1M", "$2.3M", "$2.8M", "$3.1M", "+15.2%"],
            ["Healthcare", "$1.8M", "$1.9M", "$2.1M", "$2.4M", "+12.8%"],
            ["Finance", "$1.2M", "$1.4M", "$1.6M", "$1.8M", "+18.9%"],
            ["Education", "$0.9M", "$1.0M", "$1.2M", "$1.3M", "+22.1%"],
            ["Total", "$6.0M", "$6.6M", "$7.7M", "$8.6M", "+16.7%"]
        ]

        revenue_table = Table(revenue_data, colWidths=[80, 60, 60, 60, 60, 70])
        revenue_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.navy),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 9),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, -1), (-1, -1), colors.lightgrey),
            ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ]))

        elements.append(revenue_table)
        elements.append(Spacer(1, 30))

        # Expense breakdown
        elements.append(Paragraph("Operating Expenses Analysis", styles['Heading2']))
        elements.append(Spacer(1, 10))

        expense_data = [
            ["Category", "Budget", "Actual", "Variance", "% of Revenue"],
            ["Personnel", "$3.2M", "$3.1M", "-$0.1M", "36.0%"],
            ["Technology", "$1.5M", "$1.7M", "+$0.2M", "19.8%"],
            ["Marketing", "$0.8M", "$0.9M", "+$0.1M", "10.5%"],
            ["Operations", "$1.0M", "$0.8M", "-$0.2M", "9.3%"],
            ["Legal & Compliance", "$0.3M", "$0.4M", "+$0.1M", "4.7%"],
            ["Other", "$0.5M", "$0.6M", "+$0.1M", "7.0%"],
            ["Total Expenses", "$7.3M", "$7.5M", "+$0.2M", "87.2%"]
        ]

        expense_table = Table(expense_data, colWidths=[90, 55, 55, 55, 70])
        expense_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.darkgreen),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 8),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
            ('BACKGROUND', (0, -1), (-1, -1), colors.lightgreen),
            ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
            ('ALTERNATEROWCOLOR', (0, 1), (-1, -2), [colors.white, colors.lightyellow]),
        ]))

        elements.append(expense_table)
        elements.append(Spacer(1, 30))

        # Performance metrics
        elements.append(Paragraph("Key Performance Indicators", styles['Heading2']))
        elements.append(Spacer(1, 10))

        kpi_data = [
            ["Metric", "Q4 2024", "Q4 2023", "Change", "Target", "Status"],
            ["Revenue Growth", "16.7%", "12.3%", "+4.4pp", "15.0%", "✓ Exceeded"],
            ["Profit Margin", "12.8%", "11.5%", "+1.3pp", "12.0%", "✓ Met"],
            ["Customer Acquisition", "2,340", "1,890", "+450", "2,200", "✓ Exceeded"],
            ["Employee Retention", "94.2%", "91.8%", "+2.4pp", "93.0%", "✓ Exceeded"],
            ["Market Share", "18.5%", "16.2%", "+2.3pp", "18.0%", "✓ Exceeded"],
            ["Customer Satisfaction", "4.7/5.0", "4.5/5.0", "+0.2", "4.5", "✓ Exceeded"]
        ]

        kpi_table = Table(kpi_data, colWidths=[80, 55, 55, 50, 50, 70])
        kpi_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.purple),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 8),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
            ('ALTERNATEROWCOLOR', (0, 1), (-1, -1), [colors.white, colors.lavender]),
        ]))

        elements.append(kpi_table)

        doc.build(elements)
        return filepath

    def _create_presentation_doc(self) -> str:
        """Create mixed content presentation-style document"""

        filepath = os.path.join(self.output_dir, "presentation_mixed.pdf")
        c = canvas.Canvas(filepath, pagesize=letter)
        width, height = letter

        # Create sample chart image
        img_path = os.path.join(self.output_dir, "growth_chart.png")
        img = PILImage.new("RGB", (350, 250), color="white")
        draw = ImageDraw.Draw(img)

        # Draw axes
        draw.line([(50, 200), (300, 200)], fill="black", width=2)  # X-axis
        draw.line([(50, 50), (50, 200)], fill="black", width=2)   # Y-axis

        # Draw bars
        bars = [(70, 180, 90, 200), (110, 160, 130, 200), (150, 140, 170, 200),
                (190, 120, 210, 200), (230, 100, 250, 200), (270, 80, 290, 200)]
        colors_list = ["red", "orange", "yellow", "green", "blue", "purple"]

        for i, (x1, y1, x2, y2) in enumerate(bars):
            draw.rectangle([x1, y1, x2, y2], fill=colors_list[i])

        # Labels
        draw.text((50, 210), "Revenue Growth Trend", fill="black")
        draw.text((60, 220), "Q1", fill="black")
        draw.text((120, 220), "Q2", fill="black")
        draw.text((160, 220), "Q3", fill="black")
        draw.text((200, 220), "Q4", fill="black")

        img.save(img_path)

        # Page 1: Title slide
        c.setFont("Helvetica-Bold", 24)
        c.drawString(50, height-100, "Business Performance")
        c.drawString(50, height-130, "Analysis & Strategy")

        c.setFont("Helvetica", 14)
        c.drawString(50, height-180, "Quarterly Review Meeting")
        c.drawString(50, height-200, "December 2024")

        c.setFont("Helvetica-Oblique", 12)
        c.drawString(50, height-240, "Presented by: Strategic Planning Team")

        # Bullet points
        c.setFont("Helvetica-Bold", 16)
        c.drawString(50, height-300, "Agenda Overview:")

        c.setFont("Helvetica", 12)
        agenda_items = [
            "• Performance Metrics Review",
            "• Market Analysis & Trends",
            "• Strategic Initiatives Update",
            "• Financial Projections",
            "• Risk Assessment",
            "• Action Items & Next Steps"
        ]

        y_pos = height - 330
        for item in agenda_items:
            c.drawString(70, y_pos, item)
            y_pos -= 25

        # New page
        c.showPage()

        # Page 2: Chart and analysis
        c.setFont("Helvetica-Bold", 18)
        c.drawString(50, height-50, "Revenue Performance Analysis")

        # Insert chart
        c.drawImage(img_path, 50, height-350, width=350, height=250)

        # Analysis text
        c.setFont("Helvetica-Bold", 14)
        c.drawString(50, height-380, "Key Insights:")

        c.setFont("Helvetica", 11)
        insights = [
            "• Consistent quarter-over-quarter growth of 15-20%",
            "• Q4 shows exceptional performance with 25% increase",
            "• Technology division leading growth initiatives",
            "• Market expansion contributing to revenue uplift"
        ]

        y_pos = height - 405
        for insight in insights:
            c.drawString(70, y_pos, insight)
            y_pos -= 20

        # Recommendations box
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, height-500, "Strategic Recommendations:")

        c.rect(45, height-580, 500, 70, stroke=1, fill=0)

        c.setFont("Helvetica", 10)
        recommendations = [
            "1. Accelerate investment in high-growth technology segment",
            "2. Expand market presence in emerging geographical regions",
            "3. Strengthen strategic partnerships for enhanced market reach",
            "4. Optimize operational efficiency to maintain profit margins"
        ]

        y_pos = height - 520
        for rec in recommendations:
            c.drawString(55, y_pos, rec)
            y_pos -= 15

        c.save()
        return filepath

# Cell 8: Main Processing Pipeline
def run_hybrid_extraction_pipeline():
    """Run the complete hybrid extraction pipeline"""

    # Initialize system
    config = DocumentConfig()
    processor = HybridProcessor(config)

    # Generate test PDFs
    print("=== GENERATING TEST PDFs ===")
    generator = TestPDFGenerator()
    test_files = generator.generate_all_test_pdfs()

    print(f"\n=== PROCESSING {len(test_files)} DOCUMENTS ===")

    # Process each document
    results = {}
    processing_stats = {
        'total_processed': 0,
        'regex_primary': 0,
        'ml_ensemble': 0,
        'fallback_used': 0,
        'high_confidence': 0,
        'medium_confidence': 0,
        'low_confidence': 0
    }

    for pdf_path in test_files:
        print(f"\nProcessing: {os.path.basename(pdf_path)}")

        # Process document
        result = processor.process_document(pdf_path)

        # Store results
        base_name = os.path.splitext(os.path.basename(pdf_path))[0]
        results[base_name] = result

        # Update statistics
        processing_stats['total_processed'] += 1

        if 'regex' in result.processing_method:
            processing_stats['regex_primary'] += 1
        elif 'ml' in result.processing_method:
            processing_stats['ml_ensemble'] += 1

        if 'fallback' in result.processing_method:
            processing_stats['fallback_used'] += 1

        if result.confidence >= config.CONFIDENCE_HIGH:
            processing_stats['high_confidence'] += 1
        elif result.confidence >= config.CONFIDENCE_MEDIUM:
            processing_stats['medium_confidence'] += 1
        else:
            processing_stats['low_confidence'] += 1

        # Save individual results
        save_extraction_results(base_name, result)

        print(f"  ✓ Method: {result.processing_method}")
        print(f"  ✓ Confidence: {result.confidence:.2f}")
        print(f"  ✓ Characters: {len(result.characters)}")
        print(f"  ✓ Lines: {len(result.lines)}")
        print(f"  ✓ Tables: {len(result.tables)}")
        print(f"  ✓ Structure elements: {len(result.structure)}")

    # Generate comprehensive analysis
    print(f"\n=== ANALYSIS & REPORTING ===")
    analysis_results = generate_comprehensive_analysis(results, processing_stats)

    return results, analysis_results

def save_extraction_results(base_name: str, result: ExtractionResult):
    """Save extraction results to files"""

    # Save structured data to CSV
    if result.characters:
        pd.DataFrame(result.characters).to_csv(f"{base_name}_characters.csv", index=False)

    if result.blocks:
        pd.DataFrame(result.blocks).to_csv(f"{base_name}_blocks.csv", index=False)

    if result.lines:
        pd.DataFrame(result.lines).to_csv(f"{base_name}_lines.csv", index=False)

    if result.tables:
        # Flatten table data for CSV export
        table_rows = []
        for table in result.tables:
            if isinstance(table.get('data'), list):
                for row_idx, row in enumerate(table['data']):
                    if isinstance(row, list):
                        for col_idx, cell in enumerate(row):
                            table_rows.append({
                                'table_id': table.get('table_id', ''),
                                'page': table.get('page', 0),
                                'row': row_idx,
                                'col': col_idx,
                                'content': str(cell) if cell is not None else ''
                            })
        if table_rows:
            pd.DataFrame(table_rows).to_csv(f"{base_name}_tables.csv", index=False)

    # Save structure analysis as JSON
    structure_data = {
        'processing_method': result.processing_method,
        'confidence': result.confidence,
        'structure': result.structure,
        'metadata': result.metadata
    }

    with open(f"{base_name}_structure.json", 'w') as f:
        json.dump(structure_data, f, indent=2, default=str)

def generate_comprehensive_analysis(results: Dict, stats: Dict) -> Dict:
    """Generate comprehensive analysis and visualizations"""

    # Prepare analysis data
    analysis_data = []

    for doc_name, result in results.items():
        analysis_data.append({
            'document': doc_name,
            'processing_method': result.processing_method,
            'confidence': result.confidence,
            'characters': len(result.characters),
            'blocks': len(result.blocks),
            'lines': len(result.lines),
            'tables': len(result.tables),
            'images': len(result.images),
            'pages': result.metadata.get('page_count', 0),
            'structure_type': result.metadata.get('structure_type', 'unknown'),
            'complexity_score': result.metadata.get('complexity_score', 0)
        })

    df_analysis = pd.DataFrame(analysis_data)

    # Create visualizations
    plt.figure(figsize=(20, 15))

    # Processing method distribution
    plt.subplot(3, 4, 1)
    method_counts = df_analysis['processing_method'].value_counts()
    plt.pie(method_counts.values, labels=method_counts.index, autopct='%1.1f%%')
    plt.title('Processing Method Distribution')

    # Confidence distribution
    plt.subplot(3, 4, 2)
    plt.hist(df_analysis['confidence'], bins=10, alpha=0.7, color='skyblue')
    plt.xlabel('Confidence Score')
    plt.ylabel('Frequency')
    plt.title('Confidence Distribution')

    # Content extraction comparison
    plt.subplot(3, 4, 3)
    content_cols = ['characters', 'blocks', 'lines', 'tables']
    bottom = np.zeros(len(df_analysis))

    for col in content_cols:
        plt.bar(df_analysis['document'], df_analysis[col], bottom=bottom,
                label=col, alpha=0.8)
        bottom += df_analysis[col]

    plt.xlabel('Documents')
    plt.ylabel('Count')
    plt.title('Content Extraction by Document')
    plt.xticks(rotation=45)
    plt.legend()

    # Structure type vs complexity
    plt.subplot(3, 4, 4)
    for struct_type in df_analysis['structure_type'].unique():
        subset = df_analysis[df_analysis['structure_type'] == struct_type]
        plt.scatter(subset['complexity_score'], subset['confidence'],
                   label=struct_type, alpha=0.7, s=100)

    plt.xlabel('Complexity Score')
    plt.ylabel('Confidence')
    plt.title('Structure Type vs Complexity')
    plt.legend()

    # Performance by document type
    plt.subplot(3, 4, 5)
    perf_by_type = df_analysis.groupby('structure_type')['confidence'].mean()
    plt.bar(perf_by_type.index, perf_by_type.values, color='lightgreen')
    plt.xlabel('Structure Type')
    plt.ylabel('Average Confidence')
    plt.title('Performance by Document Type')
    plt.xticks(rotation=45)

    # Processing time simulation (placeholder)
    plt.subplot(3, 4, 6)
    # Simulate processing times based on method and complexity
    df_analysis['proc_time'] = (
        df_analysis['complexity_score'] * 100 +
        df_analysis['characters'] / 10000 +
        (df_analysis['processing_method'].str.contains('ml') * 50)
    )

    plt.scatter(df_analysis['characters'], df_analysis['proc_time'],
               c=df_analysis['confidence'], cmap='viridis', alpha=0.7, s=100)
    plt.colorbar(label='Confidence')
    plt.xlabel('Character Count')
    plt.ylabel('Processing Time (simulated)')
    plt.title('Processing Efficiency')

    # Feature extraction success rates
    plt.subplot(3, 4, 7)
    feature_success = {
        'Characters': (df_analysis['characters'] > 0).sum(),
        'Lines': (df_analysis['lines'] > 0).sum(),
        'Tables': (df_analysis['tables'] > 0).sum(),
        'Blocks': (df_analysis['blocks'] > 0).sum()
    }

    plt.bar(feature_success.keys(), feature_success.values(), color='coral')
    plt.ylabel('Documents with Feature')
    plt.title('Feature Extraction Success')
    plt.xticks(rotation=45)

    # Confidence vs content richness
    plt.subplot(3, 4, 8)
    df_analysis['content_richness'] = (
        df_analysis['tables'] * 3 +
        df_analysis['images'] * 2 +
        df_analysis['lines'] / 100
    )

    plt.scatter(df_analysis['content_richness'], df_analysis['confidence'],
               alpha=0.7, s=100, c='purple')
    plt.xlabel('Content Richness Score')
    plt.ylabel('Confidence')
    plt.title('Content Richness vs Confidence')

    # Method effectiveness
    plt.subplot(3, 4, 9)
    method_conf = df_analysis.groupby('processing_method')['confidence'].agg(['mean', 'std'])
    plt.bar(method_conf.index, method_conf['mean'],
           yerr=method_conf['std'], alpha=0.7, color='lightblue')
    plt.xlabel('Processing Method')
    plt.ylabel('Average Confidence')
    plt.title('Method Effectiveness')
    plt.xticks(rotation=45)

    # Document complexity distribution
    plt.subplot(3, 4, 10)
    plt.hist(df_analysis['complexity_score'], bins=8, alpha=0.7, color='gold')
    plt.xlabel('Complexity Score')
    plt.ylabel('Frequency')
    plt.title('Document Complexity Distribution')

    # Summary statistics heatmap
    plt.subplot(3, 4, 11)
    corr_cols = ['confidence', 'complexity_score', 'characters', 'lines', 'tables']
    corr_matrix = df_analysis[corr_cols].corr()

    im = plt.imshow(corr_matrix, cmap='coolwarm', aspect='auto')
    plt.colorbar(im)
    plt.xticks(range(len(corr_cols)), corr_cols, rotation=45)
    plt.yticks(range(len(corr_cols)), corr_cols)
    plt.title('Feature Correlation Matrix')

    # Overall system performance
    plt.subplot(3, 4, 12)
    performance_metrics = [
        stats['high_confidence'] / stats['total_processed'],
        stats['medium_confidence'] / stats['total_processed'],
        stats['low_confidence'] / stats['total_processed']
    ]

    plt.pie(performance_metrics, labels=['High Conf', 'Med Conf', 'Low Conf'],
           autopct='%1.1f%%', colors=['green', 'yellow', 'red'])
    plt.title('Overall System Performance')

    plt.tight_layout()
    plt.savefig('hybrid_extraction_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Generate summary report
    summary = {
        'processing_stats': stats,
        'performance_metrics': {
            'avg_confidence': df_analysis['confidence'].mean(),
            'avg_characters_extracted': df_analysis['characters'].mean(),
            'avg_lines_reconstructed': df_analysis['lines'].mean(),
            'table_detection_rate': (df_analysis['tables'] > 0).mean(),
            'structure_detection_success': (df_analysis['confidence'] > 0.7).mean()
        },
        'recommendations': generate_recommendations(df_analysis, stats)
    }

    # Save comprehensive results
    df_analysis.to_csv('hybrid_extraction_analysis.csv', index=False)

    with open('hybrid_extraction_summary.json', 'w') as f:
        json.dump(summary, f, indent=2, default=str)

    # Print summary
    print("=== PROCESSING SUMMARY ===")
    print(f"Total documents processed: {stats['total_processed']}")
    print(f"Regex-based processing: {stats['regex_primary']}")
    print(f"ML ensemble processing: {stats['ml_ensemble']}")
    print(f"Fallback methods used: {stats['fallback_used']}")
    print(f"High confidence results: {stats['high_confidence']}")
    print(f"Average confidence: {df_analysis['confidence'].mean():.3f}")
    print(f"Average characters extracted: {df_analysis['characters'].mean():.0f}")
    print(f"Total tables detected: {df_analysis['tables'].sum()}")

    return summary

def generate_recommendations(df_analysis: pd.DataFrame, stats: Dict) -> List[str]:
    """Generate system improvement recommendations"""

    recommendations = []

    # Confidence-based recommendations
    low_conf_rate = stats['low_confidence'] / stats['total_processed']
    if low_conf_rate > 0.2:
        recommendations.append(
            f"Consider improving ML models - {low_conf_rate:.1%} of documents had low confidence"
        )

    # Method effectiveness
    if stats['fallback_used'] > 0:
        recommendations.append(
            f"Fallback methods used {stats['fallback_used']} times - investigate primary method failures"
        )

    # Feature extraction
    avg_tables = df_analysis['tables'].mean()
    if avg_tables < 1 and any('table' in doc for doc in df_analysis['document']):
        recommendations.append("Table detection may need improvement for table-heavy documents")

    avg_lines = df_analysis['lines'].mean()
    if avg_lines < 50:
        recommendations.append("Line reconstruction efficiency could be enhanced")

    # Performance optimization
    high_complexity = (df_analysis['complexity_score'] > 0.7).sum()
    if high_complexity > 0:
        recommendations.append(
            f"{high_complexity} documents had high complexity - consider specialized processing"
        )

    return recommendations

# Cell 9: Run Complete Pipeline
if __name__ == "__main__":
    print("🚀 Starting Advanced Hybrid PDF Extraction System")
    print("=" * 60)

    try:
        # Run the complete pipeline
        results, analysis = run_hybrid_extraction_pipeline()

        print("\n" + "=" * 60)
        print("✅ EXTRACTION PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 60)

        print("\n📁 Generated Files:")
        print("- Individual CSV files for each document (characters, blocks, lines, tables)")
        print("- Structure analysis JSON files")
        print("- hybrid_extraction_analysis.csv (comprehensive analysis)")
        print("- hybrid_extraction_summary.json (processing summary)")
        print("- hybrid_extraction_analysis.png (visualizations)")

        print("\n🔍 Key Features Demonstrated:")
        print("✓ Document classification by page count and structure type")
        print("✓ Adaptive processing strategy selection")
        print("✓ Regex-based processing for numbered documents")
        print("✓ ML ensemble for unstructured documents")
        print("✓ Multi-level fallback system")
        print("✓ Document-adaptive feature engineering")
        print("✓ Comprehensive table and image detection")
        print("✓ Advanced visualization and analysis")

        print("\n🎯 System Performance:")
        total_docs = analysis['processing_stats']['total_processed']
        high_conf = analysis['processing_stats']['high_confidence']
        avg_conf = analysis['performance_metrics']['avg_confidence']

        print(f"- Processed {total_docs} documents successfully")
        print(f"- {high_conf}/{total_docs} achieved high confidence ({high_conf/total_docs:.1%})")
        print(f"- Average confidence score: {avg_conf:.3f}")
        print(f"- Table detection rate: {analysis['performance_metrics']['table_detection_rate']:.1%}")

        print("\n💡 System Recommendations:")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"{i}. {rec}")

        print("\n🚀 Ready for ML Pipeline Integration!")
        print("All extracted data is now formatted and ready for:")
        print("- Machine learning model training")
        print("- Document structure classification")
        print("- Content analysis and understanding")
        print("- Advanced document processing workflows")

    except Exception as e:
        print(f"\n❌ Pipeline Error: {e}")
        import traceback
        traceback.print_exc()

# Cell 10: Additional Utility Functions
def download_results():
    """Prepare all results for download"""

    import glob

    # Find all generated files
    csv_files = glob.glob("*.csv")
    json_files = glob.glob("*.json")
    image_files = glob.glob("*.png")
    pdf_files = glob.glob("hybrid_test_pdfs/*.pdf")

    all_files = csv_files + json_files + image_files + pdf_files

    print("📦 Available files for download:")
    print(f"📊 CSV Files ({len(csv_files)}):")
    for f in csv_files[:10]:  # Show first 10
        print(f"  - {f}")
    if len(csv_files) > 10:
        print(f"  ... and {len(csv_files) - 10} more")

    print(f"\n📋 JSON Files ({len(json_files)}):")
    for f in json_files:
        print(f"  - {f}")

    print(f"\n🖼️  Image Files ({len(image_files)}):")
    for f in image_files:
        print(f"  - {f}")

    print(f"\n📄 Test PDF Files ({len(pdf_files)}):")
    for f in pdf_files:
        print(f"  - {os.path.basename(f)}")

    print(f"\n📁 Total files: {len(all_files)}")

    return all_files

def quick_document_preview(pdf_path: str, max_lines: int = 20):
    """Quick preview of document content"""

    print(f"\n📄 Document Preview: {os.path.basename(pdf_path)}")
    print("-" * 50)

    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"Pages: {len(pdf.pages)}")

            # Extract first page text
            if pdf.pages:
                first_page_text = pdf.pages[0].extract_text() or ""
                lines = first_page_text.split('\n')[:max_lines]

                print("\nFirst few lines:")
                for i, line in enumerate(lines, 1):
                    if line.strip():
                        print(f"{i:2d}: {line[:80]}{'...' if len(line) > 80 else ''}")



                if len(first_page_text.split('\n')) > max_lines:
                    extra_lines = len(first_page_text.split('\n')) - max_lines
                    print(f"{extra_lines} more lines")


    except Exception as e:
        print(f"Error previewing document: {e}")

# Cell 11: Execute Pipeline
print("🎯 Advanced Hybrid PDF Extraction System - Ready to Execute!")
print("📝 This system implements the enhanced hybrid approach with:")
print("  • Document classification by structure and complexity")
print("  • Adaptive processing strategy selection")
print("  • Multi-level fallback mechanisms")
print("  • Document-adaptive feature engineering")
print("  • Comprehensive ML ensemble processing")
print("  • Advanced table and image detection")
print("  • Detailed analysis and visualization")
print("\n▶️  Run the pipeline by executing the cells above!")
print("📁 All results will be saved as CSV, JSON, and visualization files.")
print("🚀 Perfect for ML model training and document analysis workflows!")

import os
import stat

def print_file_attributes(directory):
    """Prints attributes for all files in a directory and its subdirectories."""
    print(f"Scanning directory: {directory}")
    for root, _, files in os.walk(directory):
        for name in files:
            filepath = os.path.join(root, name)
            try:
                file_stat = os.stat(filepath)
                print(f"\nFile: {filepath}")
                print(f"  Size: {file_stat.st_size} bytes")
                print(f"  Permissions: {stat.filemode(file_stat.st_mode)}")
                print(f"  Owner UID: {file_stat.st_uid}")
                print(f"  Group GID: {file_stat.st_gid}")
                print(f"  Last accessed: {file_stat.st_atime}")
                print(f"  Last modified: {file_stat.st_mtime}")
                print(f"  Created (ctime): {file_stat.st_ctime}")
            except FileNotFoundError:
                print(f"\nFile not found: {filepath}")
            except Exception as e:
                print(f"\nError accessing file {filepath}: {e}")

# Get the current working directory
current_directory = os.getcwd()

# Print attributes for files in the current directory and subdirectories
print_file_attributes(current_directory)

import os
import pandas as pd
import glob

def print_csv_columns(directory):
    """Prints column names for all CSV files in a directory and its subdirectories."""
    print(f"Scanning directory for CSV files: {directory}")
    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)

    if not csv_files:
        print("No CSV files found.")
        return

    print(f"Found {len(csv_files)} CSV files.")

    for filepath in csv_files:
        try:
            # Read only the header to get column names efficiently
            df = pd.read_csv(filepath, nrows=0)
            print(f"\nFile: {filepath}")
            print("Columns:", df.columns.tolist())
        except Exception as e:
            print(f"\nError reading file {filepath}: {e}")

# Get the current working directory
current_directory = os.getcwd()

# Print columns for all CSV files
print_csv_columns(current_directory)